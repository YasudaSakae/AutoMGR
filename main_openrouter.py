import json
import os
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()

# --- CONFIGURA√á√ïES ---
ARQUIVO_DADOS = "dados.json"
ARQUIVO_PROMPT = "prompt_template.txt"
MARCA_SEPARADOR = "___SEPARADOR___"
OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")

TIMEOUT_SEGUNDOS = 600
MAX_TOKENS = 16000
TENTATIVAS_MAX = 3

# === LISTA DE MODELOS (ATUALIZADA) ===
MODELOS_DISPONIVEIS = {
    "1": {
        "nome": "DeepSeek V3 (Custo-Benef√≠cio)",
        "slug": "deepseek/deepseek-chat",
        "desc": "O equil√≠brio perfeito. Intelig√™ncia de ponta com pre√ßo baixo."
    },
    "2": {
        "nome": "Qwen 2.5 72B Instruct (O Mais Barato)",
        "slug": "qwen/qwen-2.5-72b-instruct",
        "desc": "Modelo da Alibaba. Imbat√≠vel em pre√ßo para tarefas de formata√ß√£o/JSON."
    },
    "3": {
        "nome": "Llama 3.3 70B (Meta)",
        "slug": "meta-llama/llama-3.3-70b-instruct",
        "desc": "O padr√£o do mercado ocidental. Muito est√°vel."
    },
    "4": {
        "nome": "DeepSeek R1 (Racioc√≠nio)",
        "slug": "deepseek/deepseek-r1",
        "desc": "Pensa antes de responder. Use para l√≥gica complexa."
    },
    "5": {
        "nome": "Kimi K2 Thinking (Racioc√≠nio)",
        "slug": "moonshotai/kimi-k2-thinking",
        "desc": "Modelo chin√™s que 'pensa' passo-a-passo. √ìtimo para contextos longos."
    },
    "6": {
        "nome": "Kimi K2 Standard (R√°pido)",
        "slug": "moonshotai/kimi-k2",
        "desc": "Vers√£o padr√£o do Kimi. Mais r√°pida e est√°vel que a vers√£o Thinking."
    }
}

# --- FUN√á√ïES DE DADOS (Mantidas) ---
CHAVES_IGNORAR = ["id", "fk_processo", "active", "order", "code", "created_at", "updated_at"]

def carregar_montar_prompt():
    print(f"üìÇ Lendo arquivos...")
    if not os.path.exists(ARQUIVO_DADOS):
        print(f"‚ùå '{ARQUIVO_DADOS}' n√£o encontrado.")
        return None, None

    with open(ARQUIVO_DADOS, 'r', encoding='utf-8') as f: dados = json.load(f)
    with open(ARQUIVO_PROMPT, 'r', encoding='utf-8') as f: template = f.read()

    def limpar(d):
        if isinstance(d, dict): return {k: limpar(v) for k, v in d.items() if k not in CHAVES_IGNORAR and v is not None and v != "null"}
        if isinstance(d, list): return [i for i in [limpar(x) for x in d] if i]
        return d

    if MARCA_SEPARADOR in template:
        sys_txt, user_txt = template.split(MARCA_SEPARADOR)
    else:
        sys_txt, user_txt = "", template

    sys_txt, user_txt = sys_txt.strip(), user_txt.strip()

    for k, v in dados.get("metadados", {}).items():
        user_txt = user_txt.replace(f"{{{{{k}}}}}", str(v))

    user_txt = user_txt.replace("{{ETP_CONTEUDO}}", json.dumps(limpar(dados.get("etp_conteudo", "")), ensure_ascii=False))
    user_txt = user_txt.replace("{{TR_CONTEUDO}}", json.dumps(limpar(dados.get("tr_conteudo", "")), ensure_ascii=False))

    return sys_txt, user_txt

# --- FUN√á√ÉO DE CHAMADA (CORRIGIDA) ---
def chamar_ia(modelo_info, system_prompt, user_prompt):
    print(f"\nüöÄ Iniciando conex√£o com: {modelo_info['nome']}")
    print(f"   Slug: {modelo_info['slug']}")

    if not OPENROUTER_API_KEY:
        print("‚ùå Erro: Configure a OPENROUTER_API_KEY no arquivo .env")
        return

    client = OpenAI(
        base_url="https://openrouter.ai/api/v1",
        api_key=OPENROUTER_API_KEY,
    )

    # L√≥gica inteligente para definir par√¢metros
    # Se for modelo de racioc√≠nio (Kimi Thinking, R1), N√ÉO usa penalidade de repeti√ß√£o
    is_reasoning_model = "thinking" in modelo_info['slug'] or "r1" in modelo_info['slug']

    params = {
        "model": modelo_info['slug'],
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
        "temperature": 0.6 if is_reasoning_model else 0.2, # Racioc√≠nio precisa de mais "liberdade"
        "max_tokens": MAX_TOKENS,
        "timeout": TIMEOUT_SEGUNDOS,
        "stream": True
    }

    # S√≥ adiciona penalidade se N√ÉO for modelo de racioc√≠nio
    if not is_reasoning_model:
        params["frequency_penalty"] = 0.3
        params["presence_penalty"] = 0.3

    texto_completo = ""

    try:
        if is_reasoning_model:
            print("üß† Modelo de racioc√≠nio detectado: Ajustando par√¢metros para evitar travamento...")

        print("‚è≥ Aguardando resposta...\n")
        print("-" * 40)

        stream = client.chat.completions.create(
            extra_headers={
                "HTTP-Referer": "https://automgr.local",
                "X-Title": "AutoMGR Script",
            },
            **params # Desempacota os par√¢metros configurados acima
        )

        for chunk in stream:
            if chunk.choices[0].delta.content is not None:
                pedaco = chunk.choices[0].delta.content
                print(pedaco, end="", flush=True)
                texto_completo += pedaco

        print("\n" + "-" * 40)

        if not texto_completo.strip():
            print("\n‚ö†Ô∏è AVISO: O modelo retornou uma resposta vazia.")
        else:
            safe_name = modelo_info['slug'].split("/")[-1].replace("-", "_").replace(".", "")
            filename = f"resultado_{safe_name}.md"

            with open(filename, "w", encoding="utf-8") as f:
                f.write(texto_completo)

            print(f"\n‚úÖ Sucesso! Resposta salva em '{filename}'")

    except Exception as e:
        print(f"\n‚ùå Erro durante a gera√ß√£o: {e}")

if __name__ == "__main__":
    sys, user = carregar_montar_prompt()

    if sys and user:
        print(f"üìù Prompt pronto ({len(user)} caracteres).")
        print("\n=== MENU ATUALIZADO (6 OP√á√ïES) ===")
        for k, v in MODELOS_DISPONIVEIS.items():
            print(f"{k}) {v['nome']}")

        escolha = input("\nDigite o n√∫mero (ou 'todas'): ").strip()

        if escolha == 'todas':
            for key in MODELOS_DISPONIVEIS:
                chamar_ia(MODELOS_DISPONIVEIS[key], sys, user)
        elif escolha in MODELOS_DISPONIVEIS:
            chamar_ia(MODELOS_DISPONIVEIS[escolha], sys, user)
        else:
            print("Op√ß√£o inv√°lida.")
